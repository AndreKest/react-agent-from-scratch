2024-11-08 11:57:47,163 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 2 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,174 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 4 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,193 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 6 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,206 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 8 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,218 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 10 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,233 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 12 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,246 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 14 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,259 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 16 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,266 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 18 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,277 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 20 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,290 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 22 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,300 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 24 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,314 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 26 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,326 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 28 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,329 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 30 is using dense attention since it is divisible by 2
2024-11-08 11:57:47,331 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 32 is using dense attention since it is divisible by 2
2024-11-08 11:58:51,761 [INFO] [io] [/src/utils/io.py]: Content written to file: ./data/output/trace_Microsoft-Phi-3-small.txt
2024-11-08 11:58:51,761 [INFO] [io] [/src/utils/io.py]: Content written to file: ./data/output/trace_Microsoft-Phi-3-small.txt
2024-11-08 11:58:51,761 [INFO] [agent] [/src/agent.py]: Starting iteration 1
2024-11-08 11:58:51,762 [INFO] [io] [/src/utils/io.py]: Content written to file: ./data/output/trace_Microsoft-Phi-3-small.txt
2024-11-08 12:01:28,047 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 2 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,050 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 4 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,053 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 6 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,055 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 8 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,058 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 10 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,060 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 12 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,068 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 14 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,071 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 16 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,074 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 18 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,076 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 20 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,079 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 22 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,081 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 24 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,084 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 26 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,087 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 28 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,089 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 30 is using dense attention since it is divisible by 2
2024-11-08 12:01:28,091 [INFO] [modeling_phi3_small] [/home/jovyan/_huggingface-shared/modules/transformers_modules/microsoft/Phi-3-small-128k-instruct/ad85cab62be398dc90203c4377a4ccbf090fbb36/modeling_phi3_small.py]: Layer 32 is using dense attention since it is divisible by 2
2024-11-08 12:02:34,015 [INFO] [io] [/src/utils/io.py]: Content written to file: ./data/output/trace_Microsoft-Phi-3-small.txt
2024-11-08 12:02:34,015 [INFO] [io] [/src/utils/io.py]: Content written to file: ./data/output/trace_Microsoft-Phi-3-small.txt
2024-11-08 12:02:34,015 [INFO] [agent] [/src/agent.py]: Starting iteration 1
2024-11-08 12:02:34,015 [INFO] [io] [/src/utils/io.py]: Content written to file: ./data/output/trace_Microsoft-Phi-3-small.txt
